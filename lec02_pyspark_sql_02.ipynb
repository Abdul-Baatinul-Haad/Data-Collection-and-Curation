{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lec02_pyspark_sql_02.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null #download and install openjdk\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz #download spark binary (gunzip)\n",
        "!tar xf spark-3.2.1-bin-hadoop2.7.tgz #extract the spark package\n",
        "!pip install -q findspark #install the findspark package"
      ],
      "metadata": {
        "id": "znvn7k0fyjVx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set environment variables\n",
        "import os \n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "JlLnD_tjmC27"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfvlC_wUG5GW",
        "outputId": "0b670548-7974-40c6-c7a3-af448e8b0746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-3.2.1-bin-hadoop2.7\tspark-3.2.1-bin-hadoop2.7.tgz\n"
          ]
        }
      ],
      "source": [
        "!ls #check directory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls sample_data #check directory under sample_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeljcuSxHDGj",
        "outputId": "1202cb47-4bda-4584-e0fb-0e1455c30ba3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anscombe.json\t\t      mnist_test.csv\n",
            "california_housing_test.csv   mnist_train_small.csv\n",
            "california_housing_train.csv  README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "sc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "PDtDnE43IcFW",
        "outputId": "e3c9a142-c5bc-4d5c-ad69-a89adcd23a3c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://5c85bbe27c5f:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "# spark = SparkSession.builder.getOrCreate() \n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark SQL basic example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()\n",
        "    \n",
        "spark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "4s4UHveKcvlK",
        "outputId": "83477957-649b-47d7-bf8f-eb6d26dbaafa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ff85baf2790>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://5c85bbe27c5f:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls spark-3.2.1-bin-hadoop2.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYiam0NioADp",
        "outputId": "4ef5f256-b48b-42ca-da78-cc61930125a5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin   data\tjars\t    LICENSE   NOTICE  R\t\t RELEASE  yarn\n",
            "conf  examples\tkubernetes  licenses  python  README.md  sbin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls spark-3.2.1-bin-hadoop2.7/examples\t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDqEOtfHoGQl",
        "outputId": "eacf8abc-04f5-4a50-8426-e2d7a691a032"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jars  src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head spark-3.2.1-bin-hadoop2.7/examples/src/main/resources/people.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcV_KJ7wqfAu",
        "outputId": "9410387c-75de-420e-8871-c3fd21358e80"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"name\":\"Michael\"}\n",
            "{\"name\":\"Andy\", \"age\":30}\n",
            "{\"name\":\"Justin\", \"age\":19}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read json file\n",
        "df = spark.read.json(\"spark-3.2.1-bin-hadoop2.7/examples/src/main/resources/people.json\")\n",
        "# Displays the content of the DataFrame to stdout\n",
        "df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Empom_anLLtk",
        "outputId": "bce32b39-d343-4617-e923-6f788491001a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+\n",
            "| age|   name|\n",
            "+----+-------+\n",
            "|null|Michael|\n",
            "|  30|   Andy|\n",
            "|  19| Justin|\n",
            "+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAu8_zbmJRw3",
        "outputId": "440e8456-1a62-4b72-d2d0-fd082ffcd07a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQeHvrAze4LP",
        "outputId": "d6bcf442-ddc5-4460-ae4c-ffdd9b84ba6c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only the \"name\" column\n",
        "df.select(\"name\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYzmerydilXy",
        "outputId": "ed885f53-6eb1-4925-d910-b7294d4588b1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|   name|\n",
            "+-------+\n",
            "|Michael|\n",
            "|   Andy|\n",
            "| Justin|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select everybody, but increment the age by 1\n",
        "df.select(df['name'], df['age'] + 1).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InxGfWaGsYJk",
        "outputId": "0e2d62f2-1140-4b6f-e30a-25a97d944450"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|   name|(age + 1)|\n",
            "+-------+---------+\n",
            "|Michael|     null|\n",
            "|   Andy|       31|\n",
            "| Justin|       20|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select people older than 21\n",
        "df.filter(df['age'] > 21).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTTmZEzTsYXR",
        "outputId": "2f5908a0-f210-4320-b3f4-27bd498fe583"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "|age|name|\n",
            "+---+----+\n",
            "| 30|Andy|\n",
            "+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count people by age\n",
        "df.groupBy(\"age\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT3Se501spd1",
        "outputId": "3253da8c-6988-45a5-f71f-bd412f9bca71"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "| age|count|\n",
            "+----+-----+\n",
            "|  19|    1|\n",
            "|null|    1|\n",
            "|  30|    1|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.take(5) #first 5 row objects"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOhxmcjcsm-3",
        "outputId": "e0e1f346-0fa7-4c27-933f-0f2db563211e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(age=None, name='Michael'),\n",
              " Row(age=30, name='Andy'),\n",
              " Row(age=19, name='Justin')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(df.take(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CUntQu7Rsre",
        "outputId": "b25fa8e5-9a9a-4c5e-89b5-492f4bb1e136"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the DataFrame as a SQL temporary view\n",
        "df.createOrReplaceTempView(\"people\")\n",
        "\n",
        "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
        "sqlDF.show()"
      ],
      "metadata": {
        "id": "1Rp0jVknJwTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79bc6864-1ce9-406c-8b38-7187a87fae90"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+\n",
            "| age|   name|\n",
            "+----+-------+\n",
            "|null|Michael|\n",
            "|  30|   Andy|\n",
            "|  19| Justin|\n",
            "+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(df))\n",
        "print(type(sqlDF))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDY-PsyJt_zc",
        "outputId": "5e1f4915-4c2d-43ad-8c2f-1666be49a090"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n",
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the DataFrame as a global temporary view\n",
        "df.createGlobalTempView(\"people\")\n",
        "\n",
        "# Global temporary view is tied to a system preserved database `global_temp`\n",
        "spark.sql(\"SELECT * FROM global_temp.people\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGh71wUTRh2S",
        "outputId": "2ad428da-b69f-49c2-de36-b8e935cf73ef"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+\n",
            "| age|   name|\n",
            "+----+-------+\n",
            "|null|Michael|\n",
            "|  30|   Andy|\n",
            "|  19| Justin|\n",
            "+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Global temporary view is cross-session\n",
        "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d09pPOliMGTK",
        "outputId": "10c4a041-5cfc-4aac-fb13-3103082ccae2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+\n",
            "| age|   name|\n",
            "+----+-------+\n",
            "|null|Michael|\n",
            "|  30|   Andy|\n",
            "|  19| Justin|\n",
            "+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head spark-3.2.1-bin-hadoop2.7/examples/src/main/resources/people.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swqIY1n0vdgD",
        "outputId": "af7efcef-1921-45ca-cb6c-57be4ef67187"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Michael, 29\n",
            "Andy, 30\n",
            "Justin, 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Load a text file and convert each line to a Row.\n",
        "lines = sc.textFile(\"spark-3.2.1-bin-hadoop2.7/examples/src/main/resources/people.txt\")\n",
        "parts = lines.map(lambda l: l.split(\",\"))\n",
        "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
        "\n"
      ],
      "metadata": {
        "id": "ekMeFHXmvIdv"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEWZIr-dvsOQ",
        "outputId": "92f9b5a1-4f14-45c5-cbb1-82ccdebc3a6e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(parts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peJkwXNpv2v7",
        "outputId": "65512481-9252-46a7-ab9b-4b42aa791b2e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.PipelinedRDD"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0Y-GH90v2yr",
        "outputId": "5dcde40b-5a63-4b18-da82-d4f743800bc2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[69] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Infer the schema, and register the DataFrame as a table.\n",
        "schemaPeople = spark.createDataFrame(people)\n",
        "schemaPeople.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8YW2vbcvsQ4",
        "outputId": "1804656c-690e-4b81-b161-2cddeb5b2c32"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   name|age|\n",
            "+-------+---+\n",
            "|Michael| 29|\n",
            "|   Andy| 30|\n",
            "| Justin| 19|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schemaPeople.createOrReplaceTempView(\"people\")\n",
        "\n",
        "# SQL can be run over DataFrames that have been registered as a table.\n",
        "teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
        "\n",
        "# The results of SQL queries are Dataframe objects.\n",
        "type(teenagers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGLIp5aSvsid",
        "outputId": "c7cb11c1-26ba-476e-9f3b-25eb2ce850c0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
        "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
        "for name in teenNames:\n",
        "    print(name)\n",
        "# Name: Justin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs4mQpSlwQq_",
        "outputId": "4b36ed58-ecdf-4654-8d96-912360b2128d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: Justin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data types\n",
        "from pyspark.sql.types import StringType, StructType, StructField\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Load a text file and convert each line to a Row.\n",
        "lines = sc.textFile(\"spark-3.2.1-bin-hadoop2.7/examples/src/main/resources/people.txt\")\n",
        "parts = lines.map(lambda l: l.split(\",\"))\n",
        "# Each line is converted to a tuple.\n",
        "people = parts.map(lambda p: (p[0], p[1].strip()))\n",
        "\n",
        "# The schema is encoded in a string.\n",
        "schemaString = \"name age\"\n",
        "\n",
        "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
        "schema = StructType(fields)\n",
        "\n",
        "# Apply the schema to the RDD.\n",
        "schemaPeople = spark.createDataFrame(people, schema)\n",
        "\n",
        "# Creates a temporary view using the DataFrame\n",
        "schemaPeople.createOrReplaceTempView(\"people\")\n",
        "\n",
        "# SQL can be run over DataFrames that have been registered as a table.\n",
        "results = spark.sql(\"SELECT name FROM people\")\n",
        "\n",
        "results.show()\n",
        "# +-------+\n",
        "# |   name|\n",
        "# +-------+\n",
        "# |Michael|\n",
        "# |   Andy|\n",
        "# | Justin|\n",
        "# +-------+"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-hMeLpowQoo",
        "outputId": "1109426c-4e85-45cc-e970-593384a6e366"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|   name|\n",
            "+-------+\n",
            "|Michael|\n",
            "|   Andy|\n",
            "| Justin|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read parquet files\n",
        "df = spark.read.load(\"spark-3.2.1-bin-hadoop2.7/examples/src/main/resources/users.parquet\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmPGCcr9xPbP",
        "outputId": "bedd6438-f426-441a-87e5-83ef8d250be7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------+----------------+\n",
            "|  name|favorite_color|favorite_numbers|\n",
            "+------+--------------+----------------+\n",
            "|Alyssa|          null|  [3, 9, 15, 20]|\n",
            "|   Ben|           red|              []|\n",
            "+------+--------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# export parquet files\n",
        "df.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")"
      ],
      "metadata": {
        "id": "C0qIYbDLyVF9"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read json and export parquet files\n",
        "df = spark.read.load(\"spark-3.2.1-bin-hadoop2.7/examples/src/main/resources/people.json\", format=\"json\")\n",
        "df.show()\n",
        "df.select(\"name\", \"age\").write.save(\"namesAndAges.parquet\", format=\"parquet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43b_H7zAybRR",
        "outputId": "25c71d8f-1c68-49a6-d639-0678b17af110"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+\n",
            "| age|   name|\n",
            "+----+-------+\n",
            "|null|Michael|\n",
            "|  30|   Andy|\n",
            "|  19| Justin|\n",
            "+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read orc and export orc files\n",
        "df = spark.read.orc(\"spark-3.2.1-bin-hadoop2.7/examples/src/main/resources/users.orc\")\n",
        "(df.write.format(\"orc\")\n",
        "    .option(\"orc.bloom.filter.columns\", \"favorite_color\")\n",
        "    .option(\"orc.dictionary.key.threshold\", \"1.0\")\n",
        "    .option(\"orc.column.encoding.direct\", \"name\")\n",
        "    .save(\"users_with_options.orc\"))"
      ],
      "metadata": {
        "id": "rYqHxHj-0G9H"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}