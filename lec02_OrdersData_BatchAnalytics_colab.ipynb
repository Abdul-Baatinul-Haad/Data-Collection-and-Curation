{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get update #update linux\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null #download and install openjdk\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz #download spark binary (gunzip). -q: Turn off Wgetâ€™s output.\n",
        "!tar xf spark-3.2.1-bin-hadoop2.7.tgz #extract the spark package\n",
        "!pip install -q findspark #install the findspark package"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Directory for uploaded files",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "6dde38c8-259c-420e-81ee-f0b9041fb70f"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URS874vC6eTI",
        "outputId": "4f6acd3c-8bff-48a2-c69c-cc5daea8d760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Connecting to security.\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Waiting for headers] [C\r0% [1 InRelease gpgv 1,581 B] [Connecting to archive.ubuntu.com (185.125.190.36\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [932 kB]\n",
            "Get:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,765 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [22.8 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,503 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [966 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,277 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,199 kB]\n",
            "Get:21 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [44.3 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [29.8 kB]\n",
            "Fetched 12.0 MB in 4s (3,365 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "#set environment variables\n",
        "import os \n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "RaWvAacQ6ndV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From Spark 2.0, SparkSession provides a common entry point for a Spark application\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate() \n",
        "spark.conf.set('spark.sql.shuffle.partitions', 6)\n",
        "# spark.conf.set('spark.executor.memory', '2g')\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "eiMBjZVH60Xe",
        "outputId": "731a9a4f-3b0a-48ae-96b8-0de8e6dfa56c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f09631eb690>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e742d8839a83:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv4xa7eP8TUm",
        "outputId": "b169f77c-0251-4bf5-da6a-c1d97c6430a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-3.2.1-bin-hadoop2.7\tspark-3.2.1-bin-hadoop2.7.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls sample_data  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSCAN0kW8cQw",
        "outputId": "24df6607-90e8-4458-9621-f33790ac3509"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anscombe.json\t\t      mnist_test.csv\t     README.md\n",
            "california_housing_test.csv   mnist_train_small.csv\n",
            "california_housing_train.csv  orders_data.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q sample_data/orders_data.zip -d sample_data/orders_data"
      ],
      "metadata": {
        "id": "m6ODI1Xo85nZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls sample_data/orders_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEdHIsor9c4F",
        "outputId": "75849490-d40f-4bb1-91e5-69610e5e89bc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orders_100.csv\torders_28.csv  orders_46.csv  orders_64.csv  orders_82.csv\n",
            "orders_10.csv\torders_29.csv  orders_47.csv  orders_65.csv  orders_83.csv\n",
            "orders_11.csv\torders_2.csv   orders_48.csv  orders_66.csv  orders_84.csv\n",
            "orders_12.csv\torders_30.csv  orders_49.csv  orders_67.csv  orders_85.csv\n",
            "orders_13.csv\torders_31.csv  orders_4.csv   orders_68.csv  orders_86.csv\n",
            "orders_14.csv\torders_32.csv  orders_50.csv  orders_69.csv  orders_87.csv\n",
            "orders_15.csv\torders_33.csv  orders_51.csv  orders_6.csv   orders_88.csv\n",
            "orders_16.csv\torders_34.csv  orders_52.csv  orders_70.csv  orders_89.csv\n",
            "orders_17.csv\torders_35.csv  orders_53.csv  orders_71.csv  orders_8.csv\n",
            "orders_18.csv\torders_36.csv  orders_54.csv  orders_72.csv  orders_90.csv\n",
            "orders_19.csv\torders_37.csv  orders_55.csv  orders_73.csv  orders_91.csv\n",
            "orders_1.csv\torders_38.csv  orders_56.csv  orders_74.csv  orders_92.csv\n",
            "orders_20.csv\torders_39.csv  orders_57.csv  orders_75.csv  orders_93.csv\n",
            "orders_21.csv\torders_3.csv   orders_58.csv  orders_76.csv  orders_94.csv\n",
            "orders_22.csv\torders_40.csv  orders_59.csv  orders_77.csv  orders_95.csv\n",
            "orders_23.csv\torders_41.csv  orders_5.csv   orders_78.csv  orders_96.csv\n",
            "orders_24.csv\torders_42.csv  orders_60.csv  orders_79.csv  orders_97.csv\n",
            "orders_25.csv\torders_43.csv  orders_61.csv  orders_7.csv   orders_98.csv\n",
            "orders_26.csv\torders_44.csv  orders_62.csv  orders_80.csv  orders_99.csv\n",
            "orders_27.csv\torders_45.csv  orders_63.csv  orders_81.csv  orders_9.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "file_location = \"sample_data/orders_data/orders_*.csv\"\n",
        "\n",
        "# create schema\n",
        "ordersSchema = StructType([\n",
        "  StructField(\"Order_ID\", DoubleType(), True),\n",
        "  StructField(\"Country\", StringType(), True),\n",
        "  StructField(\"Province\", StringType(), True),\n",
        "  StructField(\"City\", StringType(), True),\n",
        "  StructField(\"Latitude\", DoubleType(), True),\n",
        "  StructField(\"Longitude\", DoubleType(), True),\n",
        "  StructField(\"TimeStamp\", StringType(), True),\n",
        "  StructField(\"Sales_Volume\", DoubleType(), True)])\n",
        "\n",
        "# create DataFrame \n",
        "data = (\n",
        "  spark\n",
        "    .read\n",
        "    .schema(ordersSchema)\n",
        "    .csv(file_location)\n",
        ")\n",
        "\n",
        "\n",
        "# create table for SQL analytics\n",
        "data.createOrReplaceTempView(\"orders\")\n",
        "\n",
        "data.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Import Data",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "892fca5b-6efd-4e04-a8ea-fbcf01688881"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mIAP-t96eTL",
        "outputId": "004204d3-cae2-4a80-e0c3-8ccbe6845084"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+--------+--------------------+---------+---------+-------------------+------------+\n",
            "|Order_ID|Country|Province|                City| Latitude|Longitude|          TimeStamp|Sales_Volume|\n",
            "+--------+-------+--------+--------------------+---------+---------+-------------------+------------+\n",
            "|231542.0| Canada|      AB|             Calgary|-113.9835|-113.9389|2022/04/22 08:28:49|       41.49|\n",
            "|473450.0| Canada|      AB|            Edmonton|-113.4467|-113.3654|2022/04/22 05:04:24|       48.52|\n",
            "|376604.0| Canada|      AB|        Medicine Hat|-110.5798|-110.4884|2022/04/22 18:14:14|       60.79|\n",
            "|440105.0| Canada|      AB|       Sherwood Park|-113.2427| -113.242|2022/04/22 11:27:20|       77.81|\n",
            "|483058.0| Canada|      AB|            Beaumont|-113.3783|-113.2894|2022/04/22 21:04:24|       12.06|\n",
            "|303271.0| Canada|      AB|            Edmonton|-113.3609|-113.3519|2022/04/22 06:32:45|        99.0|\n",
            "|444942.0| Canada|      AB|       Fort Mcmurray|-111.4019|-111.4018|2022/04/22 10:44:56|       11.83|\n",
            "|477498.0| Canada|      NB|Macdougall Settle...| -64.7586| -64.6739|2022/04/22 04:11:29|        13.5|\n",
            "|250410.0| Canada|      AB|       Sherwood Park|-113.2142|-113.1836|2022/04/22 10:51:42|       48.38|\n",
            "|317257.0| Canada|      AB|         Fort Mackay|-111.4923|-111.4908|2022/04/22 03:45:16|       11.91|\n",
            "|271051.0| Canada|      ON|              Milton| -79.8018| -79.7156|2022/04/22 02:51:53|       80.74|\n",
            "|242527.0| Canada|      AB|             Calgary|-113.9316|-113.8966|2022/04/22 09:42:40|        61.4|\n",
            "|363883.0| Canada|      ON|            Brampton|  -79.712| -79.6918|2022/04/22 23:28:53|       68.76|\n",
            "|432813.0| Canada|      ON|       Richmond Hill| -79.3341|  -79.325|2022/04/22 15:50:34|       59.05|\n",
            "|449879.0| Canada|      QC|               Laval| -73.7235| -73.6431|2022/04/22 22:59:29|       92.37|\n",
            "|455167.0| Canada|      NS|              Sydney|  -60.119| -60.1075|2022/04/22 18:59:53|       89.23|\n",
            "|219972.0| Canada|      ON|       Richmond Hill| -79.3696| -79.2793|2022/04/22 14:34:03|        5.22|\n",
            "|475590.0| Canada|      ON|             Windsor| -83.0281| -82.9607|2022/04/22 20:11:19|        1.35|\n",
            "|213890.0| Canada|      BC|            Richmond|-122.9574|-122.8935|2022/04/22 18:11:25|        8.37|\n",
            "|302898.0| Canada|      QC|        Charlesbourg| -71.2374| -71.1506|2022/04/22 01:54:45|       30.06|\n",
            "+--------+-------+--------+--------------------+---------+---------+-------------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "df_sql = spark.sql(\"SELECT count(*) FROM orders\")\n",
        "df_sql.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "438d9de3-16e5-4b38-a538-6a7e21b20c79"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4OI-TYT6eTN",
        "outputId": "0a8f278c-e33c-439f-9822-83e8d7d38d75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|   20000|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "df_sql = spark.sql(\"\"\"\n",
        "select \n",
        "  Province,\n",
        "  round(sum(Sales_Volume)) as Sales_Volume\n",
        "from orders\n",
        "group by Province\n",
        "order by Province\n",
        "\"\"\")\n",
        "df_sql.show()\n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "SQL Query & Visualize",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "e0a33156-44a6-4bb9-b803-411070400cd1"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXbMTQEk6eTO",
        "outputId": "08400ab1-6343-42b8-d2c3-0d79b7e2a9d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------+\n",
            "|Province|Sales_Volume|\n",
            "+--------+------------+\n",
            "|      AB|    397173.0|\n",
            "|      BC|     37501.0|\n",
            "|      MB|     13708.0|\n",
            "|      NB|     11314.0|\n",
            "|      NL|      2864.0|\n",
            "|      NS|      8899.0|\n",
            "|      NT|       137.0|\n",
            "|      ON|    431850.0|\n",
            "|      PE|        41.0|\n",
            "|      QC|     73407.0|\n",
            "|      SK|     37867.0|\n",
            "|      YT|       132.0|\n",
            "+--------+------------+\n",
            "\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "fcc55e84-2ce1-4272-8ebe-be5c9dfc01a9"
        },
        "id": "4ylGtR8q6eTP"
      },
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Orders Data - Batch Analytics",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 1469406854226421
    },
    "colab": {
      "name": "lec02_OrdersData_BatchAnalytics_colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}