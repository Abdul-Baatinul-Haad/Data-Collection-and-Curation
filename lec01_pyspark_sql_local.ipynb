{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "znvn7k0fyjVx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is WinOS\n",
      " Volume Serial Number is D643-BD11\n",
      "\n",
      " Directory of C:\\Users\\user\\OneDrive\\BDAT1008\\week2\n",
      "\n",
      "2022-05-18  10:38 PM    <DIR>          .\n",
      "2022-05-18  10:38 PM    <DIR>          ..\n",
      "2022-05-18  05:45 PM    <DIR>          .ipynb_checkpoints\n",
      "2022-05-17  09:13 PM               718 derby.log\n",
      "2022-05-17  07:59 PM            62,542 lec01_pyspark_csv_windows.ipynb\n",
      "2022-05-18  10:38 PM            34,803 lec01_pyspark_sql_windows - Copy.ipynb\n",
      "2022-05-17  10:39 PM            34,699 lec01_pyspark_sql_windows.ipynb\n",
      "2022-05-17  09:21 PM           703,722 lec01_pyspark_sql_windows_stages.pdf\n",
      "2022-05-15  11:43 PM            27,677 lec02_pyspark_sql_02.ipynb\n",
      "2022-05-17  09:13 PM    <DIR>          metastore_db\n",
      "2022-05-15  10:54 PM               723 part-00000-0231aeb9-684e-4c0c-af85-da4ecf779a08-c000.snappy.parquet\n",
      "2022-05-15  11:19 PM             1,050 part-00000-21705c6e-4696-4621-b88a-531ebde2ae3a-c000.snappy.orc\n",
      "2022-05-17  09:18 PM    <DIR>          sample_data\n",
      "2022-05-18  04:48 PM           647,067 Spark Environment Variable Setup v1.0.docx\n",
      "               9 File(s)      1,513,001 bytes\n",
      "               5 Dir(s)  21,160,263,680 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JlLnD_tjmC27"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-OMSTOP3.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1448dd04340>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From Spark 2.0, SparkSession provides a common entry point for a Spark application\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate() \n",
    "spark.conf.set('spark.sql.shuffle.partitions', 6)\n",
    "spark.conf.set('num-executors', 16)\n",
    "# spark.conf.set('spark.executor.memory', '2g')\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KfvlC_wUG5GW",
    "outputId": "a514044a-cb34-475b-80af-4d770ad6847c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is WinOS\n",
      " Volume Serial Number is D643-BD11\n",
      "\n",
      " Directory of C:\\Users\\user\\OneDrive\\BDAT1008\\week2\\sample_data\n",
      "\n",
      "2022-05-17  08:01 PM    <DIR>          .\n",
      "2022-05-17  08:01 PM    <DIR>          ..\n",
      "2022-05-02  10:40 PM           301,141 california_housing_test.csv\n",
      "2022-05-02  10:40 PM         1,706,430 california_housing_train.csv\n",
      "2022-05-02  10:40 PM        18,289,443 mnist_test.csv\n",
      "               3 File(s)     20,297,014 bytes\n",
      "\n",
      " Directory of C:\\Users\\user\\OneDrive\\BDAT1008\\week2\n",
      "\n",
      "\n",
      " Directory of C:\\Users\\user\\OneDrive\\BDAT1008\\week2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File Not Found\n"
     ]
    }
   ],
   "source": [
    "%ls sample_data #check directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Empom_anLLtk",
    "outputId": "6762d0f6-c85d-4194-9984-f99e4caf8ea1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(longitude=-114.31, latitude=34.19, housing_median_age=15.0, total_rooms=5612.0, total_bedrooms=1283.0, population=1015.0, households=472.0, median_income=1.4936, median_house_value=66900.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate() \n",
    "\n",
    "# File location and type\n",
    "path = 'sample_data/california_housing_train.csv'\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# Import csv. The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(path)\n",
    "\n",
    "df.head() #first row object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "IAu8_zbmJRw3",
    "outputId": "e0c8df0d-cba7-4050-af72-64bfc6458279"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_house_value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "UQeHvrAze4LP",
    "outputId": "316e0ed5-0eed-4068-e1b9-b4fcdbab9a18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JYzmerydilXy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1Rp0jVknJwTJ"
   },
   "outputs": [],
   "source": [
    "# create a temporary view\n",
    "df.createOrReplaceTempView(\"california_housing_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "wGh71wUTRh2S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|  -114.31|   34.19|              15.0|     5612.0|        1283.0|    1015.0|     472.0|       1.4936|           66900.0|\n",
      "|  -114.47|    34.4|              19.0|     7650.0|        1901.0|    1129.0|     463.0|         1.82|           80100.0|\n",
      "|  -114.56|   33.69|              17.0|      720.0|         174.0|     333.0|     117.0|       1.6509|           85700.0|\n",
      "|  -114.57|   33.64|              14.0|     1501.0|         337.0|     515.0|     226.0|       3.1917|           73400.0|\n",
      "|  -114.57|   33.57|              20.0|     1454.0|         326.0|     624.0|     262.0|        1.925|           65500.0|\n",
      "|  -114.58|   33.63|              29.0|     1387.0|         236.0|     671.0|     239.0|       3.3438|           74000.0|\n",
      "|  -114.58|   33.61|              25.0|     2907.0|         680.0|    1841.0|     633.0|       2.6768|           82400.0|\n",
      "|  -114.59|   34.83|              41.0|      812.0|         168.0|     375.0|     158.0|       1.7083|           48500.0|\n",
      "|  -114.59|   33.61|              34.0|     4789.0|        1175.0|    3134.0|    1056.0|       2.1782|           58400.0|\n",
      "|   -114.6|   34.83|              46.0|     1497.0|         309.0|     787.0|     271.0|       2.1908|           48100.0|\n",
      "|   -114.6|   33.62|              16.0|     3741.0|         801.0|    2434.0|     824.0|       2.6797|           86500.0|\n",
      "|   -114.6|    33.6|              21.0|     1988.0|         483.0|    1182.0|     437.0|        1.625|           62000.0|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT * FROM california_housing_train LIMIT 12\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "d09pPOliMGTK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "pOhxmcjcsm-3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(longitude=-114.31, latitude=34.19, housing_median_age=15.0, total_rooms=5612.0, total_bedrooms=1283.0, population=1015.0, households=472.0, median_income=1.4936, median_house_value=66900.0),\n",
       " Row(longitude=-114.47, latitude=34.4, housing_median_age=19.0, total_rooms=7650.0, total_bedrooms=1901.0, population=1129.0, households=463.0, median_income=1.82, median_house_value=80100.0),\n",
       " Row(longitude=-114.56, latitude=33.69, housing_median_age=17.0, total_rooms=720.0, total_bedrooms=174.0, population=333.0, households=117.0, median_income=1.6509, median_house_value=85700.0),\n",
       " Row(longitude=-114.57, latitude=33.64, housing_median_age=14.0, total_rooms=1501.0, total_bedrooms=337.0, population=515.0, households=226.0, median_income=3.1917, median_house_value=73400.0),\n",
       " Row(longitude=-114.57, latitude=33.57, housing_median_age=20.0, total_rooms=1454.0, total_bedrooms=326.0, population=624.0, households=262.0, median_income=1.925, median_house_value=65500.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql.take(5) #first 5 row objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2CUntQu7Rsre"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "88CPzxrssjL3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+----------+\n",
      "|longitude|latitude|population|households|\n",
      "+---------+--------+----------+----------+\n",
      "|  -114.31|   34.19|    1015.0|     472.0|\n",
      "|  -114.47|    34.4|    1129.0|     463.0|\n",
      "|  -114.56|   33.69|     333.0|     117.0|\n",
      "|  -114.57|   33.64|     515.0|     226.0|\n",
      "|  -114.57|   33.57|     624.0|     262.0|\n",
      "|  -114.58|   33.63|     671.0|     239.0|\n",
      "|  -114.58|   33.61|    1841.0|     633.0|\n",
      "|  -114.59|   34.83|     375.0|     158.0|\n",
      "|  -114.59|   33.61|    3134.0|    1056.0|\n",
      "|   -114.6|   34.83|     787.0|     271.0|\n",
      "+---------+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT longitude, latitude, population, households FROM california_housing_train LIMIT 10\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gU3g8SqJstB6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[longitude: double, latitude: double, population: double, households: double]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "e6_0VKKzJTuw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+------+\n",
      "|  longi| lati|   pop|    hh|\n",
      "+-------+-----+------+------+\n",
      "|-114.31|34.19|1015.0| 472.0|\n",
      "|-114.47| 34.4|1129.0| 463.0|\n",
      "|-114.56|33.69| 333.0| 117.0|\n",
      "|-114.57|33.64| 515.0| 226.0|\n",
      "|-114.57|33.57| 624.0| 262.0|\n",
      "|-114.58|33.63| 671.0| 239.0|\n",
      "|-114.58|33.61|1841.0| 633.0|\n",
      "|-114.59|34.83| 375.0| 158.0|\n",
      "|-114.59|33.61|3134.0|1056.0|\n",
      "| -114.6|34.83| 787.0| 271.0|\n",
      "+-------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use sql aliases\n",
    "df_sql = spark.sql(\"SELECT longitude as longi, \\\n",
    "  latitude as lati, population as pop, \\\n",
    "  households as hh FROM california_housing_train LIMIT 10\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JyBP6lsYsX5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|  -117.12|   33.52|               4.0|    30401.0|        4957.0|   13251.0|    4339.0|       4.5841|          212300.0|\n",
      "|  -117.42|   33.35|              14.0|    25135.0|        4819.0|   35682.0|    4769.0|       2.5729|          134400.0|\n",
      "|  -117.61|    34.1|               9.0|    18956.0|        4095.0|   10323.0|    3832.0|       3.6033|          132600.0|\n",
      "|  -117.74|   33.89|               4.0|    37937.0|        5471.0|   16122.0|    5189.0|       7.4947|          366300.0|\n",
      "|  -117.75|   34.01|               4.0|    22128.0|        3522.0|   10450.0|    3258.0|       6.1287|          289600.0|\n",
      "|  -117.78|   34.03|               8.0|    32054.0|        5290.0|   15507.0|    5050.0|       6.0191|          253900.0|\n",
      "|  -117.87|   34.04|               7.0|    27700.0|        4179.0|   15037.0|    4072.0|       6.6288|          339700.0|\n",
      "|  -117.88|   33.96|              16.0|    19059.0|        3079.0|   10988.0|    3061.0|       5.5469|          265200.0|\n",
      "|  -118.09|   34.68|               4.0|    23386.0|        4171.0|   10493.0|    3671.0|       4.0211|          144000.0|\n",
      "|   -118.1|   34.57|               7.0|    20377.0|        4335.0|   11973.0|    3933.0|       3.3086|          138100.0|\n",
      "|  -118.46|    34.4|              12.0|    25957.0|        4798.0|   10475.0|    4490.0|        4.542|          195300.0|\n",
      "|  -118.78|   34.16|               9.0|    30405.0|        4093.0|   12873.0|    3931.0|       8.0137|          399200.0|\n",
      "|   -118.9|   34.26|               5.0|    25187.0|        3521.0|   11956.0|    3478.0|       6.9712|          321300.0|\n",
      "|  -120.59|    34.7|              29.0|    17738.0|        3114.0|   12427.0|    2826.0|       2.7377|           28300.0|\n",
      "|   -121.4|   38.47|               4.0|    20982.0|        3392.0|   10329.0|    3086.0|       4.3658|          130600.0|\n",
      "|  -121.61|   36.69|              19.0|     9899.0|        2617.0|   11272.0|    2528.0|       2.0244|          118500.0|\n",
      "|  -121.68|   36.72|              12.0|    19234.0|        4492.0|   12153.0|    4372.0|       3.2652|          152800.0|\n",
      "|  -121.79|   36.64|              11.0|    32627.0|        6445.0|   28566.0|    6082.0|       2.3087|          118800.0|\n",
      "|  -121.92|   37.53|               7.0|    28258.0|        3864.0|   12203.0|    3701.0|       8.4045|          451100.0|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter by population  >= 10000\n",
    "df_sql = spark.sql(\"SELECT * FROM california_housing_train WHERE population  >= 10000\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NGNQh5vMsYAs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|  -121.79|   36.64|              11.0|    32627.0|        6445.0|   28566.0|    6082.0|       2.3087|          118800.0|\n",
      "|  -117.78|   34.03|               8.0|    32054.0|        5290.0|   15507.0|    5050.0|       6.0191|          253900.0|\n",
      "|  -117.74|   33.89|               4.0|    37937.0|        5471.0|   16122.0|    5189.0|       7.4947|          366300.0|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter by population  >= 10000 and households>=5000\n",
    "df_sql = spark.sql(\"SELECT * FROM california_housing_train \\\n",
    "  WHERE population >= 10000 and households>=5000 \\\n",
    "  ORDER BY housing_median_age DESC\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "RoaHwOgfX2yw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------------+--------------+\n",
      "|longitude|latitude|min_total_bedrooms|avg_median_income|max_households|\n",
      "+---------+--------+------------------+-----------------+--------------+\n",
      "|   -124.0|    39.0|             151.0|             2.78|         803.0|\n",
      "|   -124.0|    40.0|             340.0|             1.78|         486.0|\n",
      "|   -124.0|    41.0|             238.0|             2.17|         414.0|\n",
      "|   -124.0|    42.0|             357.0|             3.05|         314.0|\n",
      "|   -123.0|    38.0|               1.0|             4.34|        2200.0|\n",
      "|   -123.0|    39.0|              17.0|             2.87|        1088.0|\n",
      "|   -123.0|    40.0|              28.0|             1.97|         627.0|\n",
      "|   -123.0|    41.0|              84.0|             2.34|         757.0|\n",
      "|   -123.0|    42.0|             308.0|             2.28|         869.0|\n",
      "|   -122.0|    37.0|               4.0|             5.93|        3589.0|\n",
      "|   -122.0|    38.0|               3.0|             4.11|        2952.0|\n",
      "|   -122.0|    39.0|              49.0|             3.18|         810.0|\n",
      "|   -122.0|    40.0|              77.0|             2.44|         809.0|\n",
      "|   -122.0|    41.0|              40.0|             2.56|        2267.0|\n",
      "|   -122.0|    42.0|             350.0|             2.38|         685.0|\n",
      "+---------+--------+------------------+-----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#aggregation\n",
    "df_sql = spark.sql(\"SELECT round(longitude,0) as longitude, round(latitude,0) as latitude, \\\n",
    "                    min(total_bedrooms) as min_total_bedrooms, round(avg(median_income),2) as avg_median_income, \\\n",
    "                    max(households) as max_households \\\n",
    "                    FROM california_housing_train \\\n",
    "                    WHERE longitude BETWEEN -124 AND -122 \\\n",
    "                    GROUP BY round(longitude,0), round(latitude,0) \\\n",
    "                    ORDER BY round(longitude,0), round(latitude,0)\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "tZ8aIv9BL5Jg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------+\n",
      "|longitude|latitude|count(1)|\n",
      "+---------+--------+--------+\n",
      "|  -124.18|   40.78|       3|\n",
      "|  -124.17|    40.8|       3|\n",
      "|  -124.16|   40.78|       3|\n",
      "|  -124.16|   40.79|       2|\n",
      "|  -124.16|    40.8|       2|\n",
      "|  -124.14|   40.59|       2|\n",
      "|  -124.13|   40.79|       2|\n",
      "|  -124.11|   40.93|       2|\n",
      "|   -124.1|    40.5|       2|\n",
      "|  -124.09|   40.88|       3|\n",
      "|  -124.07|   40.87|       2|\n",
      "|  -123.79|   39.44|       3|\n",
      "|  -123.22|   39.15|       2|\n",
      "|  -123.21|   39.15|       2|\n",
      "|  -122.92|   39.05|       2|\n",
      "|  -122.91|   39.05|       2|\n",
      "|  -122.82|   38.39|       2|\n",
      "|  -122.76|   38.44|       2|\n",
      "|  -122.75|   38.46|       2|\n",
      "|  -122.74|   38.44|       2|\n",
      "+---------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find out duplicated longitude and latitude records\n",
    "df_sql = spark.sql(\"SELECT longitude, latitude, COUNT(*) FROM california_housing_train \\\n",
    "  GROUP BY longitude, latitude \\\n",
    "  HAVING COUNT(*) > 1 \\\n",
    "  ORDER BY longitude, latitude \\\n",
    "  \")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ajlFSexkhvTc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the view if it exists\n",
    "spark.sql(\"\"\"\n",
    "  DROP VIEW IF EXISTS longitude_latitude_dup; \n",
    "\"\"\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HrbGUkYHm24-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create another temporary view\n",
    "spark.sql(\"\"\"\n",
    "  \n",
    "  CREATE TEMPORARY VIEW longitude_latitude_dup AS \n",
    "  SELECT longitude, latitude, COUNT(*) AS record_count FROM california_housing_train \n",
    "  GROUP BY longitude, latitude \n",
    "  HAVING COUNT(*) > 1 \n",
    "  ORDER BY longitude, latitude \n",
    "\"\"\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "kq7mXj-MkYwq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+\n",
      "|longitude|latitude|record_count|\n",
      "+---------+--------+------------+\n",
      "|  -124.18|   40.78|           3|\n",
      "|  -124.17|    40.8|           3|\n",
      "|  -124.16|   40.78|           3|\n",
      "|  -124.16|   40.79|           2|\n",
      "|  -124.16|    40.8|           2|\n",
      "|  -124.14|   40.59|           2|\n",
      "|  -124.13|   40.79|           2|\n",
      "|  -124.11|   40.93|           2|\n",
      "|   -124.1|    40.5|           2|\n",
      "|  -124.09|   40.88|           3|\n",
      "|  -124.07|   40.87|           2|\n",
      "|  -123.79|   39.44|           3|\n",
      "|  -123.22|   39.15|           2|\n",
      "|  -123.21|   39.15|           2|\n",
      "|  -122.92|   39.05|           2|\n",
      "|  -122.91|   39.05|           2|\n",
      "|  -122.82|   38.39|           2|\n",
      "|  -122.76|   38.44|           2|\n",
      "|  -122.75|   38.46|           2|\n",
      "|  -122.74|   38.44|           2|\n",
      "+---------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dup = spark.sql(\"SELECT * FROM longitude_latitude_dup\") \n",
    "df_dup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "SBwEhhf9kAe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|record_count|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+------------+\n",
      "|  -124.18|   40.78|              33.0|     1076.0|         222.0|     656.0|     236.0|       2.5096|           72200.0|           3|\n",
      "|  -124.18|   40.78|              34.0|     1592.0|         364.0|     950.0|     317.0|       2.1607|           67000.0|           3|\n",
      "|  -124.18|   40.78|              37.0|     1453.0|         293.0|     867.0|     310.0|       2.5536|           70200.0|           3|\n",
      "|  -124.17|    40.8|              52.0|      661.0|         316.0|     392.0|     244.0|        0.957|           60000.0|           3|\n",
      "|  -124.17|    40.8|              52.0|     1557.0|         344.0|     758.0|     319.0|       1.8529|           62500.0|           3|\n",
      "|  -124.17|    40.8|              52.0|     1606.0|         419.0|     891.0|     367.0|        1.585|           75500.0|           3|\n",
      "|  -124.16|   40.78|              43.0|     2241.0|         446.0|     932.0|     395.0|       2.9038|           82000.0|           3|\n",
      "|  -124.16|   40.78|              46.0|     1975.0|         346.0|     791.0|     349.0|          3.8|           81800.0|           3|\n",
      "|  -124.16|   40.78|              50.0|     2285.0|         403.0|     837.0|     353.0|       2.5417|           85400.0|           3|\n",
      "|  -124.16|   40.79|              52.0|     2148.0|         421.0|     975.0|     430.0|       2.2566|           92700.0|           2|\n",
      "|  -124.16|   40.79|              52.0|     1264.0|         277.0|     591.0|     284.0|       1.7778|           76900.0|           2|\n",
      "|  -124.16|    40.8|              52.0|     2416.0|         618.0|    1150.0|     571.0|       1.7308|           80500.0|           2|\n",
      "|  -124.16|    40.8|              52.0|     1703.0|         500.0|     952.0|     435.0|       1.1386|           74100.0|           2|\n",
      "|  -124.14|   40.59|              17.0|     2985.0|         610.0|    1544.0|     584.0|        2.178|           76800.0|           2|\n",
      "|  -124.14|   40.59|              22.0|     1665.0|         405.0|     826.0|     382.0|       1.5625|           66800.0|           2|\n",
      "|  -124.13|   40.79|              29.0|     2474.0|         453.0|    1130.0|     427.0|       2.8833|           83000.0|           2|\n",
      "|  -124.13|   40.79|              32.0|     2017.0|         359.0|     855.0|     346.0|       3.5833|           92800.0|           2|\n",
      "|  -124.11|   40.93|              17.0|     1661.0|         329.0|     948.0|     357.0|       2.7639|           90200.0|           2|\n",
      "|  -124.11|   40.93|              25.0|     2392.0|         474.0|    1298.0|     461.0|       3.5076|           73600.0|           2|\n",
      "|   -124.1|    40.5|              30.0|     1927.0|         393.0|     996.0|     374.0|       2.2357|           72300.0|           2|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join two views\n",
    "df_join = spark.sql(\"\"\"SELECT v1.*, v2.record_count \n",
    "                     FROM california_housing_train v1 \n",
    "                     INNER JOIN longitude_latitude_dup v2 \n",
    "                     ON v1.longitude = v2.longitude\n",
    "                     AND v1.latitude = v2.latitude\n",
    "                     ORDER BY v1.longitude, v1.latitude, housing_median_age\n",
    "                     \"\"\")\n",
    "df_join.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "gadq5N6rnZBy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|record_count|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+------------+\n",
      "|  -124.18|   40.78|              33.0|     1076.0|         222.0|     656.0|     236.0|       2.5096|           72200.0|           3|\n",
      "|  -124.17|    40.8|              52.0|      661.0|         316.0|     392.0|     244.0|        0.957|           60000.0|           3|\n",
      "|  -124.16|   40.78|              43.0|     2241.0|         446.0|     932.0|     395.0|       2.9038|           82000.0|           3|\n",
      "|  -124.16|   40.79|              52.0|     2148.0|         421.0|     975.0|     430.0|       2.2566|           92700.0|           2|\n",
      "|  -124.16|    40.8|              52.0|     2416.0|         618.0|    1150.0|     571.0|       1.7308|           80500.0|           2|\n",
      "|  -124.14|   40.59|              17.0|     2985.0|         610.0|    1544.0|     584.0|        2.178|           76800.0|           2|\n",
      "|  -124.13|   40.79|              29.0|     2474.0|         453.0|    1130.0|     427.0|       2.8833|           83000.0|           2|\n",
      "|  -124.11|   40.93|              17.0|     1661.0|         329.0|     948.0|     357.0|       2.7639|           90200.0|           2|\n",
      "|   -124.1|    40.5|              30.0|     1927.0|         393.0|     996.0|     374.0|       2.2357|           72300.0|           2|\n",
      "|  -124.09|   40.88|              26.0|     2683.0|         555.0|    1353.0|     526.0|       2.4321|           82100.0|           3|\n",
      "|  -124.07|   40.87|              31.0|      334.0|         134.0|     780.0|     130.0|       0.7684|          153100.0|           2|\n",
      "|  -123.79|   39.44|              16.0|     2017.0|         423.0|    1177.0|     414.0|       3.2171|          116200.0|           3|\n",
      "|  -123.22|   39.15|              36.0|     1166.0|         216.0|     504.0|     203.0|       3.5938|          122100.0|           2|\n",
      "|  -123.21|   39.15|              31.0|     2685.0|         675.0|    1367.0|     626.0|       1.6571|          108900.0|           2|\n",
      "|  -122.92|   39.05|              16.0|     1548.0|         295.0|     605.0|     250.0|       3.5652|          119000.0|           2|\n",
      "|  -122.91|   39.05|              20.0|     1128.0|         229.0|     621.0|     210.0|       3.2216|           93500.0|           2|\n",
      "|  -122.82|   38.39|              22.0|     1288.0|         243.0|     593.0|     220.0|        3.625|          233700.0|           2|\n",
      "|  -122.76|   38.44|              11.0|     2895.0|         524.0|    1633.0|     534.0|       4.7283|          170200.0|           2|\n",
      "|  -122.75|   38.46|              13.0|     4323.0|        1020.0|    2566.0|     728.0|       3.0147|          142800.0|           2|\n",
      "|  -122.74|   38.44|              17.0|     2287.0|         497.0|    1240.0|     493.0|       3.5845|          164300.0|           2|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop_duplicates will only keep the first row for each group of same longitude and latitude\n",
    "df_join.drop_duplicates(['longitude','latitude']).orderBy([\"longitude\", \"latitude\"], ascending=[1, 1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vaLH2ijaoH-9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lec01_pyspark_sql_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
