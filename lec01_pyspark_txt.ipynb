{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lec01_pyspark_txt.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get update #update linux\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null #download and install openjdk\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz #download spark binary (gunzip). -q: Turn off Wgetâ€™s output.\n",
        "!tar xf spark-3.2.1-bin-hadoop2.7.tgz #extract the spark package\n",
        "!pip install -q findspark #install the findspark package"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9pabsMFYasL",
        "outputId": "507eccf8-4b6f-4010-9691-05efa60dbc67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,167 kB]\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,950 kB]\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,000 kB]\n",
            "Fetched 6,388 kB in 7s (906 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /usr/lib/jvm/"
      ],
      "metadata": {
        "id": "bhAvCUJSaMqL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e9cbd0f-118b-4143-cd96-7f9ba407af99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default-java  java-1.11.0-openjdk-amd64  java-11-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /usr/lib/jvm/java-8-openjdk-amd64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o5-zbLbyT15",
        "outputId": "9c447623-0d9b-4b55-bf42-103663c16273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/usr/lib/jvm/java-8-openjdk-amd64': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#set environment variables\n",
        "import os \n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "suQ73DSOaM8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls #check directory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE82JUIqaXUI",
        "outputId": "da3ce775-b8d3-49f2-f526-8f4932b0aea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls spark-3.2.1-bin-hadoop2.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcnm5gCvaXWs",
        "outputId": "b34d804d-db3c-4c95-9bdd-bc9be65a5965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin   data\tjars\t    LICENSE   NOTICE  R\t\t RELEASE  yarn\n",
            "conf  examples\tkubernetes  licenses  python  README.md  sbin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls spark-3.2.1-bin-hadoop2.7/bin/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL6h2UuHar8U",
        "outputId": "2813f1f0-62f1-4ece-cb8c-ae7aca2e2ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beeline\t\t      pyspark\t\tspark-class.cmd   spark-sql\n",
            "beeline.cmd\t      pyspark2.cmd\tsparkR\t\t  spark-sql2.cmd\n",
            "docker-image-tool.sh  pyspark.cmd\tsparkR2.cmd\t  spark-sql.cmd\n",
            "find-spark-home       run-example\tsparkR.cmd\t  spark-submit\n",
            "find-spark-home.cmd   run-example.cmd\tspark-shell\t  spark-submit2.cmd\n",
            "load-spark-env.cmd    spark-class\tspark-shell2.cmd  spark-submit.cmd\n",
            "load-spark-env.sh     spark-class2.cmd\tspark-shell.cmd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "-zGwaIXgXbHR",
        "outputId": "7ce298c0-3e4e-4695-d163-a4b24b2f9c8a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://0499f8092905:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "sc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate() "
      ],
      "metadata": {
        "id": "swEm9j8mXq6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path='spark-3.2.1-bin-hadoop2.7/README.md'\n",
        "textFile = spark.read.text(path)"
      ],
      "metadata": {
        "id": "n8uw6OC1cpoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(textFile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25iX_yfSdoFR",
        "outputId": "b3be6678-b6fd-48c5-e650-70a993349157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textFile.count() # Number of rows in this DataFrame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0CWvGSQeIO3",
        "outputId": "388e3a7c-4767-4a3b-afa5-98e6552fbd63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "109"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textFile.first()  # First row in this DataFrame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr8gn0DrexVt",
        "outputId": "02a97f4c-fbf7-4e68-ee89-9f26e630898b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(value='# Apache Spark')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textFile.filter(textFile.value.contains(\"Spark\")).count()  # How many lines contain \"Spark\"?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyf0a7XyfLAp",
        "outputId": "537e8626-d63d-43f9-94a8-b7579a324909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(textFile.filter(textFile.value.contains(\"Spark\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4ZvzCctop7b",
        "outputId": "c2e7d47a-a2f2-4d2d-a18c-a2b623c5e1db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textFile.filter(textFile.value.contains(\"Spark\")).show(truncate=False) #truncate=False will show the full content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezHhQumAfXVX",
        "outputId": "c134e931-adf8-41fd-9a6e-2872f6f2e446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|value                                                                                                                                                        |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|# Apache Spark                                                                                                                                               |\n",
            "|Spark is a unified analytics engine for large-scale data processing. It provides                                                                             |\n",
            "|rich set of higher-level tools including Spark SQL for SQL and DataFrames,                                                                                   |\n",
            "|[![PySpark Coverage](https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/spark)                                  |\n",
            "|You can find the latest Spark documentation, including a programming                                                                                         |\n",
            "|## Building Spark                                                                                                                                            |\n",
            "|Spark is built using [Apache Maven](https://maven.apache.org/).                                                                                              |\n",
            "|To build Spark and its example programs, run:                                                                                                                |\n",
            "|[\"Building Spark\"](https://spark.apache.org/docs/latest/building-spark.html).                                                                                |\n",
            "|For general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](https://spark.apache.org/developer-tools.html).|\n",
            "|The easiest way to start using Spark is through the Scala shell:                                                                                             |\n",
            "|Spark also comes with several sample programs in the `examples` directory.                                                                                   |\n",
            "|    ./bin/run-example SparkPi                                                                                                                                |\n",
            "|    MASTER=spark://host:7077 ./bin/run-example SparkPi                                                                                                       |\n",
            "|Testing first requires [building Spark](#building-spark). Once Spark is built, tests                                                                         |\n",
            "|Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported                                                                                |\n",
            "|Hadoop, you must build Spark against the same version that your cluster runs.                                                                                |\n",
            "|in the online documentation for an overview on how to configure Spark.                                                                                       |\n",
            "|Please review the [Contribution to Spark guide](https://spark.apache.org/contributing.html)                                                                  |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find the max number of words in a line\n",
        "from pyspark.sql.functions import *\n",
        "textFile.select(size(split(textFile.value, \"\\s+\")).name(\"numWords\")).agg(max(col(\"numWords\"))).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ph-WHTtfa9x",
        "outputId": "814dc675-f4c4-42d3-f941-5dd1fdc6ed75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(max(numWords)=16)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(textFile.select(size(split(textFile.value, \"\\s+\")).name(\"numWords\")).agg(max(col(\"numWords\"))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8C5Z4GapwpY",
        "outputId": "da3e9710-edd9-4795-bdf4-e4eb0d6471d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textFile2 = sc.textFile(path) #import txt to RDD\n",
        "type(textFile2)"
      ],
      "metadata": {
        "id": "ete4ZkABp4Oy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32e6824e-a7ef-43f3-a613-7a0656b593b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textFile2.take(5) #for RDD, use 'take' to get the first few rows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38nh16R0sJ4N",
        "outputId": "d81790bf-4557-4b12-cc43-fd6cb8699e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['# Apache Spark',\n",
              " '',\n",
              " 'Spark is a unified analytics engine for large-scale data processing. It provides',\n",
              " 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
              " 'supports general computation graphs for data analysis. It also supports a']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find the max number of words in a line using map reduce\n",
        "textFile2.map(lambda line : len(line.split(\" \"))).reduce(lambda a, b : a if (a > b) else b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKJeTdb6sRcH",
        "outputId": "3f60fe37-9a47-4af0-acb1-2fb134615b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "34ZugWEXv08t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}